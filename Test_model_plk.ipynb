{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn++.NC - Test sur Nouveau Dataset\n",
    "\n",
    "**Objectif:** Charger un mod√®le entra√Æn√© (.pkl) et l'√©valuer sur un dataset totalement diff√©rent.\n",
    "\n",
    "Compatible avec:\n",
    "- `.pkl` SV Transfer (dict avec clf + scaler)\n",
    "- `.pkl` LearnPPNC (objet classique)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install optuna -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from typing import Dict, List, Tuple\n",
    "import pickle\n",
    "import warnings\n",
    "from google.colab import files\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Imports OK\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FEATURE_COLS = [\n",
    "    'Electrical speed [rad/s]',\n",
    "    'I_M_a', 'I_M_b', 'I_M_c',\n",
    "    'I_P_a', 'I_P_b', 'I_P_c',\n",
    "    'I_B_a', 'I_B_b', 'I_B_c',\n",
    "    'V_M_a', 'V_M_b', 'V_M_c',\n",
    "    'V_P_a', 'V_P_b', 'V_P_c',\n",
    "    'V_B_a', 'V_B_b', 'V_B_c'\n",
    "]\n",
    "LABEL_COL = 'Class label'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Charger le Mod√®le\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"üìÅ CHARGER LE MOD√àLE (.pkl):\")\n",
    "uploaded = files.upload()\n",
    "pkl_name = list(uploaded.keys())[0]\n",
    "\n",
    "with open(pkl_name, 'rb') as f:\n",
    "    saved = pickle.load(f)\n",
    "\n",
    "# D√©tecter le format\n",
    "if isinstance(saved, dict) and 'clf' in saved:\n",
    "    # Format SV Transfer\n",
    "    clf = saved['clf']\n",
    "    scaler = saved['scaler']\n",
    "    all_classes = np.array(saved['classes'])\n",
    "    MODEL_TYPE = 'sv_transfer'\n",
    "    print(f\"\\nMod√®le charg√©! (SV Transfer)\")\n",
    "    print(f\"  M√©thode: {saved.get('method', 'SV Transfer')}\")\n",
    "    print(f\"  Classes connues: {list(all_classes)}\")\n",
    "    print(f\"  Accuracy (train): {saved['accuracy_train']:.4f}\")\n",
    "    print(f\"  Hyperparam√®tres: C={saved['C']:.4f}, gamma={saved['gamma']:.6f}\")\n",
    "    print(f\"  Support Vectors: {saved['n_sv']}\")\n",
    "else:\n",
    "    # Format LearnPPNC classique\n",
    "    model = saved\n",
    "    clf = None\n",
    "    scaler = model.scaler\n",
    "    all_classes = np.array(sorted(model.all_classes))\n",
    "    MODEL_TYPE = 'learnpp'\n",
    "    print(f\"\\nMod√®le charg√©! (LearnPPNC)\")\n",
    "    print(f\"  Nombre d'experts: {len(model.experts)}\")\n",
    "    print(f\"  Classes connues: {list(all_classes)}\")\n",
    "    print(f\"  Hyperparam√®tres: C={model.C:.4f}, gamma={model.gamma:.6f}\")\n",
    "    for i, e in enumerate(model.experts):\n",
    "        print(f\"  Expert #{i+1}: {e['name']} - Accuracy: {e['accuracy']:.4f} - SVs: {e['n_sv']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Charger le Nouveau Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"üìÅ CHARGER LE NOUVEAU DATASET (.csv):\")\n",
    "uploaded = files.upload()\n",
    "test_file = list(uploaded.keys())[0]\n",
    "\n",
    "df_test = pd.read_csv(test_file)\n",
    "X_test = df_test[FEATURE_COLS].values\n",
    "y_test = df_test[LABEL_COL].values\n",
    "\n",
    "print(f\"\\nCharg√©: {len(y_test)} √©chantillons, {len(np.unique(y_test))} classes\")\n",
    "print(f\"Classes: {sorted(np.unique(y_test))}\")\n",
    "print(f\"\\nDistribution:\")\n",
    "for c in sorted(np.unique(y_test)):\n",
    "    print(f\"  Classe {c:2d} : {np.sum(y_test == c):4d} √©chantillons\")\n",
    "\n",
    "# V√©rifier compatibilit√©\n",
    "test_classes = sorted(np.unique(y_test))\n",
    "unknown = set(test_classes) - set(all_classes)\n",
    "if unknown:\n",
    "    print(f\"\\n‚ö†Ô∏è  Classes dans le test INCONNUES du mod√®le: {sorted(unknown)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Toutes les classes du test sont connues du mod√®le\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. √âvaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "if MODEL_TYPE == 'sv_transfer':\n",
    "    y_pred = clf.predict(X_test_sc)\n",
    "    y_proba = clf.predict_proba(X_test_sc)\n",
    "    confidence = np.max(y_proba, axis=1)\n",
    "else:\n",
    "    res = model.evaluate(X_test, y_test)\n",
    "    y_pred = res['y_pred']\n",
    "    confidence = res['confidence']\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"  ACCURACY: {acc:.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Matrice de confusion\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=all_classes)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=all_classes.astype(int),\n",
    "            yticklabels=all_classes.astype(int),\n",
    "            ax=ax, vmin=0, vmax=1,\n",
    "            cbar_kws={'label': 'Proportion'})\n",
    "\n",
    "ax.set_xlabel('Classe Pr√©dite', fontsize=12)\n",
    "ax.set_ylabel('Classe R√©elle', fontsize=12)\n",
    "ax.set_title(f\"Test sur Nouveau Dataset\\nAccuracy: {acc:.2%}\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Accuracy par classe\n",
    "class_acc = []\n",
    "class_count = []\n",
    "for c in all_classes:\n",
    "    mask = y_test == c\n",
    "    if np.sum(mask) > 0:\n",
    "        class_acc.append(np.mean(y_pred[mask] == c))\n",
    "        class_count.append(np.sum(mask))\n",
    "    else:\n",
    "        class_acc.append(0)\n",
    "        class_count.append(0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "colors = ['green' if a > 0.9 else 'orange' if a > 0.7 else 'red' for a in class_acc]\n",
    "bars = ax.bar(range(len(all_classes)), class_acc, color=colors, edgecolor='black')\n",
    "ax.axhline(acc, color='blue', ls='--', lw=2, label=f'Accuracy globale: {acc:.4f}')\n",
    "ax.set_xticks(range(len(all_classes)))\n",
    "ax.set_xticklabels([int(c) for c in all_classes])\n",
    "ax.set_xlabel('Classe')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy par Classe')\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.bar(range(len(all_classes)), class_count, color='steelblue', edgecolor='black')\n",
    "ax.set_xticks(range(len(all_classes)))\n",
    "ax.set_xticklabels([int(c) for c in all_classes])\n",
    "ax.set_xlabel('Classe')\n",
    "ax.set_ylabel(\"Nombre d'√©chantillons\")\n",
    "ax.set_title('Distribution des Classes (Test)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_class_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Distribution de confiance\n",
    "correct = y_test == y_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(confidence[correct], bins=20, alpha=0.7, label=f'Correct (n={sum(correct)})', color='green', edgecolor='black')\n",
    "ax.hist(confidence[~correct], bins=20, alpha=0.7, label=f'Incorrect (n={sum(~correct)})', color='red', edgecolor='black')\n",
    "ax.axvline(np.mean(confidence), color='blue', ls='--', label=f'Moyenne: {np.mean(confidence):.3f}')\n",
    "ax.set_xlabel('Score de Confiance')\n",
    "ax.set_ylabel('Nombre de pr√©dictions')\n",
    "ax.set_title('Distribution des Scores de Confiance')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_confidence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confiance moyenne (correct): {np.mean(confidence[correct]):.4f}\")\n",
    "print(f\"Confiance moyenne (incorrect): {np.mean(confidence[~correct]):.4f}\" if sum(~correct) > 0 else \"Tout correct!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. D√©tail des Pr√©dictions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tableau d√©taill√© des 20 premiers exemples\n",
    "n_show = min(20, len(y_test))\n",
    "X_show_sc = X_test_sc[:n_show]\n",
    "\n",
    "if MODEL_TYPE == 'sv_transfer':\n",
    "    preds = clf.predict(X_show_sc)\n",
    "    probas = clf.predict_proba(X_show_sc)\n",
    "    confs = np.max(probas, axis=1)\n",
    "    detail = pd.DataFrame({\n",
    "        'y_true': y_test[:n_show].astype(int),\n",
    "        'y_pred': preds.astype(int),\n",
    "        'confidence': np.round(confs, 4),\n",
    "        'correct': preds == y_test[:n_show]\n",
    "    })\n",
    "else:\n",
    "    detail_df = model.get_expert_predictions(X_test[:n_show])\n",
    "    detail_df['y_true'] = y_test[:n_show]\n",
    "    detail_df['y_final'] = y_pred[:n_show]\n",
    "    detail_df['confidence'] = confidence[:n_show]\n",
    "    detail = detail_df\n",
    "\n",
    "print(f\"D√©tail des pr√©dictions ({n_show} premiers exemples):\")\n",
    "detail"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. T√©l√©charger les R√©sultats\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "for f in ['test_confusion_matrix.png', 'test_class_performance.png', 'test_confidence.png']:\n",
    "    if os.path.exists(f):\n",
    "        files.download(f)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}